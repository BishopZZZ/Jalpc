---
layout: post
title:  "监督学习算法"
date:   2019-03-21
desc: "Machine learning"
keywords: "Machine Learning,website,blog,easy"
categories: [Machine learning]
tags: [Machine learning]
icon: icon-python1
---
## 监督学习算法

### **K近邻**

k-NN 算法可以说是最简单的机器学习算法。构建模型只需要保存训练数据集即可。

想要对新数据点做出预测，算法会在训练数据集中找到最近的数据点，也就是它的“最近邻”。

```py
 mglearn.plots.plot_knn_classification(n_neighbors=1)
```

除了仅考虑最近邻，我还可以考虑任意个(k 个)邻居。这也是 k 近邻算法名字的来历。 在考虑多于一个邻居的情况时，我们用“投票法”(voting)来指定标签。

也就是说，对 于每个测试点，我们数一数多少个邻居属于类别 0，多少个邻居属于类别 1。然后将出现 次数更多的类别(也就是 k 个近邻中占多数的类别)作为预测结果。

```py
 mglearn.plots.plot_knn_classification(n_neighbors=3)
```

***

### **线性模型**

线性回归，或者普通最小二乘法(ordinary least squares，OLS)，是回归问题最简单也最经典的线性方法。线性回归寻找参数 w 和 b，使得对训练集的预测值与真实的回归目标值 y 之间的均方误差最小。均方误差(mean squared error)是预测值与真实值之差的平方和除 以样本数。线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度。

```py
 from sklearn.linear_model import LinearRegression
      X, y = mglearn.datasets.make_wave(n_samples=60)
      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
lr = LinearRegression().fit(X_train, y_train)
```

***

### **朴素贝叶斯分类器**

朴素贝叶斯分类器是与线性模型非常相似的一种分类器，但它的训练速度往往更快。这种高效率所付出的代价是，朴素贝叶斯模型的泛化能力要比线性分类器(如 LogisticRegression 和 LinearSVC)稍差。

朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。scikit-learn 中实现了三种朴素贝叶斯分类器: GaussianNB、BernoulliNB 和 MultinomialNB。GaussianNB 可应用于任意连续数据，而BernoulliNB 假定输入数据为二分类数据，MultinomialNB 假定输入数据为计数数据(即每个特征代表某个对象的整数计数，比如一个单词在句子里出现的次数)。

MultinomialNB 和 BernoulliNB 都只有一个参数 alpha，用于控制模型复杂度。alpha 的工作原理是，算法向数据中添加 alpha 这么多的虚拟数据点，这些点对所有特征都取正值。这 可以将统计数据“平滑化”(smoothing)。alpha 越大，平滑化越强，模型复杂度就越低。 算法性能对 alpha 值的鲁棒性相对较好，也就是说，alpha 值对模型性能并不重要。但调 整这个参数通常都会使精度略有提高。

GaussianNB 主要用于高维数据，而另外两种朴素贝叶斯模型则广泛用于稀疏计数数据，比 如文本。MultinomialNB 的性能通常要优于 BernoulliNB，特别是在包含很多非零特征的数 据集(即大型文档)上。

***

### **决策树**

决策树是广泛用于分类和回归任务的模型。本质上，它从一层层的 if/else 问题中进行学习，并得出结论。

学习决策树，就是学习一系列 if/else 问题，使我们能够以最快的速度得到正确答案。在机器学习中，这些问题叫作测试。为了构造决策树，算法搜遍所有可能的测试，找出对目标变量来说信息量最大的那一个。

通常来说，构造决策树直到所有叶结点都是纯的叶结点，这会导致模型非常复杂，并且对训练数据高度过拟合。纯叶结点的存在说明这棵树在训练集上的精度是100%。训练集中 的每个数据点都位于分类正确的叶结点中。

防止过拟合有两种常见的策略:一种是及早停止树的生长，也叫预剪枝(pre-pruning); 另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝(post-pruning)或 剪枝(pruning)。预剪枝的限制条件可能包括限制树的最大深度、限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。

```py
from sklearn.tree import DecisionTreeClassifier
     cancer = load_breast_cancer()
     X_train, X_test, y_train, y_test = train_test_split(
         cancer.data, cancer.target, stratify=cancer.target, random_state=42)
     tree = DecisionTreeClassifier(random_state=0)
     tree.fit(X_train, y_train)
     print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))
     print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))
```

与前面讨论过的许多算法相比，决策树有两个优点:一是得到的模型很容易可视化，非专家也很容易理解(至少对于较小的树而言);二是算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好。

决策树的主要缺点在于，即使做了预剪枝，它也经常会过拟合，泛化性能很差。因此，在 大多数应用中，往往使用下面介绍的集成方法来替代单棵决策树。

***

### **核支持向量机**

核支持向量机(通常简称为 SVM)是可以推广到更复杂模型的扩展，这些模型无法被输入空间的超平面定义。线性模型在低维空间中可能非常受限，因为线和平面的灵活性有限。
有一种方法可以让线性模型更加灵活，就是添加更多的特征。

用于分类的线性模型只能用一条直线来划分数据点。

```py
from sklearn.svm import LinearSVC
      linear_svm = LinearSVC().fit(X, y)
      mglearn.plots.plot_2d_separator(linear_svm, X)
      mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
      plt.xlabel("Feature 0")
      plt.ylabel("Feature 1")
```

现在我们对输入特征进行扩展，比如说添加第二个特征的平方(feature1 ** 2)作为一个新特征。
现在我们将每个数据点表示为三维点(feature0, feature1, feature1 ** 2)，而 不是二维点 (feature0, feature1)。

```py
# 添加第二个特征的平方，作为一个新特征 X_new = np.hstack([X, X[:, 1:] ** 2])
from mpl_toolkits.mplot3d import Axes3D, axes3d figure = plt.figure()
# 3D可视化
ax = Axes3D(figure, elev=-152, azim=-26)
# 首先画出所有y == 0的点，然后画出所有y == 1的点
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
cmap=mglearn.cm2, s=60)
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
               cmap=mglearn.cm2, s=60)
     ax.set_xlabel("feature0")
     ax.set_ylabel("feature1")
     ax.set_zlabel("feature1 ** 2")
```

在数据的新表示中，现在可以用线性模型(三维空间中的平面)将这两个类别分开。如果将线性SVM模型看作原始特征的函数，那么它实际上已经不是线性的了。
它不是一条直线，而是一个椭圆。

#### 核技巧

有一种巧妙的数学技巧，让我们可以在 更高维空间中学习分类器，而不用实际计算可能非常大的新的数据表示。这种技巧叫作核技巧(kernel trick)，
它的原理是直接计算扩展特征表示中数据点之间的距离(更准确地说 是内积)，而不用实际对扩展进行计算。

#### 理解SVM

在训练过程中，SVM 学习每个训练数据点对于表示两个类别之间的决策边界的重要性。通常只有一部分训练数据点对于定义决策边界来说很重要:位于类别之间边界上的那些点。这些点叫作支持向量(support vector)，支持向量机正是由此得名。

想要对新样本点进行预测，需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性(保存在SVC的 dual_coef_ 属性中)来做出的。

核支持向量机是非常强大的模型，在各种数据集上的表现都很好。SVM 允许决策边界很复杂，即使数据只有几个特征。它在低维数据和高维数据(即很少特征和很多特征)上的表现都很好，但对样本个数的缩放表现不好。在有多达10000个样本的数据上运行SVM可能表现良好，但如果数据量达到 100000 甚至更大，在运行时间和内存使用方面可能会面临挑战。
SVM 的另一个缺点是，预处理数据和调参都需要非常小心。这也是为什么如今很多应用中用的都是基于树的模型，比如随机森林或梯度提升(需要很少的预处理，甚至不需要预 处理)。此外，SVM模型很难检查，可能很难理解为什么会这么预测，而且也难以将模型向非专家进行解释。