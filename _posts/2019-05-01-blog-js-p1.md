---
layout: post
title:  "NLP"
date:   2019-06-14
desc: "Natural Language Processing"
keywords: "Machine Learning,Deep learning,blog"
categories: [Machine learning]
tags: [Machine learning]
icon: icon-python1
---
## Natural language Processing

### Preprocessing

- Text Normalization
  - Segment Structure (sentence segmentation)
  - Tokenise words
  - Normalize words:
    - Lower casing,
    - Removing morphology(词法)
      - Lemmatization
      - Stemming
    - Correct spelling
    - Expanding abbrs
  - Remove stop words

### Vector Space Model

- Term-Doc Matrix
  - Bag of words (raw counts)
  - Rows represent docs, columns for terms
  - Similarity with cosine
  - TF-IDF: TF: Raw count, IDF: idf(t) = log N/df(t)
  - BM25
  - Query processing in VSM: Treat query as short pseudo-doc, Calculate similarity, Rank docs by cosine similarity,Return top k result

- Inverted index
  - Terms as rows +  list of (docID, weight) , aka posting list

- Vector space measure of query-docs similarity

- Efficient search for best docs

### Index Compression & Efficient Query Processing

- Posting list Compression
  - Intuition:
    - Minimize storage costs
    - Fast sequential access
    - Support GEQ(x) : smallest item in the list that is greater or equal to x
  - Store gaps
  - V-Byte Compression: encode and decode
  - OptPForDelta Compression

- Efficient Query Processing
  - BM25
  - Top-k Retrieval: WAND
    - Retrieve the top k items for a given query without having to evaluate all docs
    - Keep track the top-k highest scored docs
    - For each unique term, store the Maximum Contribution it can have to any doc
    - Skip over docs that cannot enter the top-k result

### Text Classification

- Tasks
  - Topic classification
  - Sentiment Analysis
  - Native-language Identification
  - Automatic fact-checking

- Steps:
  - Collect corpus
  - Select features
  - Choose ML algorithm
  - Tune hyperparameters
  - Repeat earlier steps
  - Train final model
  - Evaluate model

- Choosing an algorithm:
  - Bias vs. variance
  - feature independence
  - feature scaling
  - complexity
  - speed

- Algorithms:
  - Naive Bayes
  - Logistic Regression
  - SVM
  - kNN
  - Decision tree
  - Random forests

- Evaluation:
  - Accuracy
  - Precision & Recall
  - F1-Score

### Lexical Semantics

- Basic Lexical Relations
  - Synonyms and antonyms
  - Hypernyms (generic), hyponyms (specific)
  - Holoynms (whole) and meronyms (part)

- Similarity
  - Similarity with paths
  - include depth information (Wu & Palmer)
  - Lin Similarity

### Distributional Semantics

- Count based method
  - VSM
    - TF IDF
    - Dimensionality Reduction: SVD
  - Words as Context (PMI Matrix)
- Neural word embeddings
  - Skip-gram model
  - CBOW
  - Negative Sampling

### Part of Speech tagging

- Use of POS
  - Lemmatization
  - Very useful features
  - Information Extraction
  - Word sense disambiguation
  - Parsing

- Tagger
  - Unigram tagger
  - Classifier-based taggers
  - HMM tagger
  - Neural net for Tagging

### Language Model

- N-gram Language Models: Smoothing(Laplacian (add-one) smoothing, Add-k smoothing, Kneser-Ney smoothing)
- Neural Sequence Models: FFNNLM, RNN
  - RNN:
    - Start with initial hidden state h0, Embed words as inputs, Compute hidden states, Compute output - softmax
    - Change to binary - Negative Sampling
    - Variant RNNs: Bi-RNN, LSTM, GRU

### Information Extraction

- Name Entity Recogonization: IOB Tagging, NER as sequence labelling, Supervised learning - Features
- Relation Extraction
- Question Answering
  - Steps:
    - Forming query
    - Document and passage ranking
    - Answer selection

### Regular Grammar

- Regular Expression
- Properties of Regular Languages
  - Closed under Concatenation
  - Intersection
  - Negation
- Finite State Acceptors (FSA)
- Weighted FSA (WFSA)
- Finite State Transducers: Edit distance WFST, Inflectional Morphology

### Context-free grammars

- Symbols
  - Terminal
  - Non-terminal
- Productions (rules)
  - One non-terminal on LHS
  - A list of symbols on RHS
- CFG trees
  - Non-terminals are internal trees
  - Terminals are leaves
- CYK parsing algorithms (DP)
  - Convert grammar to CNF
  - Fill in a parse table
  - Use table to derive parse
  - Convert result back to original grammar

### Dependency Grammars

- Dependency
  - links between a head word and its dependent words
  - Who did what to whom
- Dependencies trees
- Dependency Parsing
  - Transition-based
    - Two data structures: Buffer, Stack
    - Two types of transitions: Shift, Arc (left/right)
  - Graph-based

### Discourse

- Discourse segmentation
  - Assumption: text can be divided into a number of
  - Task: classifying whether a boundary exists between any two sentences
  - Unsupervised approach: TextTiling Algorithm
  - Supervised approach:
    - Features: Distributional Semantics, Coreferences cues, Discourse markers
    - Binary classifier to identify boundaries
    - Sequential classifiers
- Discourse parsing
  - Intuition:
    - DU (Discourse Units) are related by specific coherence relations
    - A discourse will form a tree
    - RST (Rhetorical Structure Theory) Tree
  - Simple rule-based parser (by discourse markers)
  - Full CYK/chart - not practical
  - Greedy Approach
    - Segment into DUs
    - Classify all adjacent DUs as being same DU or not
    - Combine most probable into a single DU
    - Repeat until entire text is a single DU
  - Anaphor resolution
    - Centering Algorithm
    - Supervised Anaphor resolution
  
### Machine Translation: Word-based models

- Noisy channel Machine Translation
- Word Alignment
  - IBM Model 1
    - EM (Expectation Maximization) Algorithm
- HMM

- Phrased-based Machine Translation: Treat n-grams as translation units, referred to as 'phrases' Very complicated
- DP Solution - Similar with Viterbi
- Neural Machine Translation
  - Encoder-decoder models
  - RNN Encoder-Decoders (seq2seq)
  - RNN Attention
  - Evaluation: Fluency, Adequacy
  - Metric: BLEU
  