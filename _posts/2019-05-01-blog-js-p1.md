---
layout: post
title:  "NLP"
date:   2019-06-14
desc: "Natural Language Processing"
keywords: "Machine Learning,Deep learning,blog"
categories: [Machine learning]
tags: [Machine learning]
icon: icon-python1
---
## Natural language Processing

### Preprocessing

- Text Normalization
  - Segment Structure (sentence segmentation)
  - Tokenise words
  - Normalize words:
    - Lower casing,
    - Removing morphology(词法)
      - Lemmatization
      - Stemming
    - Correct spelling
    - Expanding abbrs
  - Remove stop words

### Vector Space Model

- Term-Doc Matrix
  - Bag of words (raw counts)
  - Rows represent docs, columns for terms
  - Similarity with cosine
  - TF-IDF: TF: Raw count, IDF: idf(t) = log N/df(t)
  - BM25
  - Query processing in VSM: Treat query as short pseudo-doc, Calculate similarity, Rank docs by cosine similarity,Return top k result

- Inverted index
  - Terms as rows +  list of (docID, weight) , aka posting list

- Vector space measure of query-docs similarity

- Efficient search for best docs

### Index Compression & Efficient Query Processing

- Posting list Compression
  - Intuition:
    - Minimize storage costs
    - Fast sequential access
    - Support GEQ(x) : smallest item in the list that is greater or equal to x
  - Store gaps
  - V-Byte Compression: encode and decode
  - OptPForDelta Compression

- Efficient Query Processing
  - BM25
  - Top-k Retrieval: WAND
    - Retrieve the top k items for a given query without having to evaluate all docs
    - Keep track the top-k highest scored docs
    - For each unique term, store the Maximum Contribution it can have to any doc
    - Skip over docs that cannot enter the top-k result

### Text Classification
